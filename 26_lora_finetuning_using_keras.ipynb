{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99333c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da88eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U keras-hub\n",
    "! pip install  -q -U keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac351b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "# https://www.kaggle.com/models/keras/gemma3\n",
    "\n",
    "# gemma_lm = keras_nlp.models.GemmaCasualLM.from_preset(\"gemma_2b_en\")\n",
    "\n",
    "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
    "\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e12d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference before fine-tuning\n",
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "sampler = keras_hub.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset jsonl file\n",
    "! wget -O databricks-dolly-15k.jsonl https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0019f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompts = []\n",
    "responses = []\n",
    "line_count = 0\n",
    "\n",
    "with open(\"databricks-dolly-15k.jsonl\") as file:\n",
    "    for line in file:\n",
    "        if line_count >= 1000:\n",
    "            break  # Limit the training examples, to reduce execution time.\n",
    "\n",
    "        examples = json.loads(line)\n",
    "        # Filter out examples with context, to keep it simple.\n",
    "        if examples[\"context\"]:\n",
    "            continue\n",
    "        # Format data into prompts and response lists.\n",
    "        prompts.append(examples[\"instruction\"])\n",
    "        responses.append(examples[\"response\"])\n",
    "\n",
    "        line_count += 1\n",
    "\n",
    "data = {\n",
    "    \"prompts\": prompts,\n",
    "    \"responses\": responses\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning\n",
    "\n",
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeedc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the input sequence length to 256 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 256\n",
    "\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e117abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the fine-tuning process\n",
    "gemma_lm.fit(data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below if you want to enable mixed precision training on GPUs\n",
    "# keras.mixed_precision.set_global_policy('mixed_bfloat16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference after fine-tuning\n",
    "prompt = template.format(\n",
    "    instruction=\"What should I do on a trip to Europe?\",\n",
    "    response=\"\",\n",
    ")\n",
    "sampler = keras_hub.samplers.TopKSampler(k=5, seed=2)\n",
    "gemma_lm.compile(sampler=sampler)\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbac5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving fine-tune results\n",
    "# For demonstration purposes, this tutorial fine-tunes the model on a small subset of the dataset for just one epoch and with a low LoRA rank value. To get better responses from the fine-tuned model, you can experiment with:\n",
    "\n",
    "# Increasing the size of the fine-tuning dataset\n",
    "# Training for more steps (epochs)\n",
    "# Setting a higher LoRA rank\n",
    "# Modifying the hyperparameter values such as learning_rate and weight_decay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
